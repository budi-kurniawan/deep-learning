Server A: Intel i9 7900 (10 cores, 20 threads), GPU GTX 1080 Ti
Server B: code Titan
NN architecture: See https://github.com/budi-kurniawan/deep-learning/blob/main/mnist-torch-gpu.py

Pytorch MNIST training, 2 hidden network
Server A: 311 seconds (CPU, multicore), 1111 seconds (CPU, single-core forcing) 108 seconds (GPU), both with loss around 0.02 and accuracy >99%
Server B: 1366 seconds (CPU, multicore), 226 seconds (GPU), both with loss around 0.02 and accuracy >99%

Note: PyTorch will optimise by using multithreads in the CPU. Forcing to use a single core is done when multiple programs need to run concurrently. To force PyTorch to use a single thread, set this env var:

OMP_NUM_THREADS = 1

Conclusion: training with GPU is faster even for relatively small NNs.
